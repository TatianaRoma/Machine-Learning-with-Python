{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#COS 575 Lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Tatiana Romanchishina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chr2015 data set, fit classification models in order to predict whether a given community has a Premature_death_value above or below the median. \n",
    "Explore logistic regression, LDA, and KNN (select a few models with different values for k) models using at least three subsets of predictors with up to a max of five predictors. Be sure to produce a confusion matrix for each of the techniques. Describe your findings. Which subset produced the best results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATECODE</th>\n",
       "      <th>COUNTYCODE</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Premature_death_Value</th>\n",
       "      <th>Poor_or_fair_health_Value</th>\n",
       "      <th>Poor_physical_health_days_Value</th>\n",
       "      <th>Poor_mental_health_days_Value</th>\n",
       "      <th>Low_birthweight_Value</th>\n",
       "      <th>Adult_smoking_Value</th>\n",
       "      <th>...</th>\n",
       "      <th>Uninsured_adults_Value</th>\n",
       "      <th>Uninsured_children_Value</th>\n",
       "      <th>Health_care_costs_Value</th>\n",
       "      <th>Could_not_see_doctor_due_to_cost_Value</th>\n",
       "      <th>Other_primary_care_providers_Value</th>\n",
       "      <th>Median_household_income_Value</th>\n",
       "      <th>Children_eligible_for_free_lunch_Value</th>\n",
       "      <th>Homicide_rate_Value</th>\n",
       "      <th>bottom10</th>\n",
       "      <th>top10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>9508</td>\n",
       "      <td>0.205</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.043</td>\n",
       "      <td>10127</td>\n",
       "      <td>0.163</td>\n",
       "      <td>48</td>\n",
       "      <td>42882</td>\n",
       "      <td>0.500</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AL</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>8405</td>\n",
       "      <td>0.228</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.037</td>\n",
       "      <td>9939</td>\n",
       "      <td>0.156</td>\n",
       "      <td>18</td>\n",
       "      <td>51868</td>\n",
       "      <td>0.383</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>AL</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>7457</td>\n",
       "      <td>0.127</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.054</td>\n",
       "      <td>9502</td>\n",
       "      <td>0.144</td>\n",
       "      <td>29</td>\n",
       "      <td>47539</td>\n",
       "      <td>0.344</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>AL</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>8901</td>\n",
       "      <td>0.234</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.045</td>\n",
       "      <td>10414</td>\n",
       "      <td>0.169</td>\n",
       "      <td>11</td>\n",
       "      <td>30981</td>\n",
       "      <td>0.697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>AL</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>10729</td>\n",
       "      <td>0.179</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.042</td>\n",
       "      <td>10825</td>\n",
       "      <td>0.163</td>\n",
       "      <td>9</td>\n",
       "      <td>39781</td>\n",
       "      <td>0.546</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATECODE  COUNTYCODE State          County  Premature_death_Value  \\\n",
       "0          1           0    AL         Alabama                   9508   \n",
       "1          1           1    AL  Autauga County                   8405   \n",
       "2          1           3    AL  Baldwin County                   7457   \n",
       "3          1           5    AL  Barbour County                   8901   \n",
       "4          1           7    AL     Bibb County                  10729   \n",
       "\n",
       "   Poor_or_fair_health_Value  Poor_physical_health_days_Value  \\\n",
       "0                      0.205                              4.3   \n",
       "1                      0.228                              5.1   \n",
       "2                      0.127                              3.3   \n",
       "3                      0.234                              4.8   \n",
       "4                      0.179                              4.7   \n",
       "\n",
       "   Poor_mental_health_days_Value  Low_birthweight_Value  Adult_smoking_Value  \\\n",
       "0                            4.3                   0.10                0.219   \n",
       "1                            3.6                   0.09                0.217   \n",
       "2                            3.8                   0.09                0.206   \n",
       "3                            4.3                   0.12                0.251   \n",
       "4                            5.1                   0.13                0.259   \n",
       "\n",
       "   ...    Uninsured_adults_Value  Uninsured_children_Value  \\\n",
       "0  ...                     0.202                     0.043   \n",
       "1  ...                     0.169                     0.037   \n",
       "2  ...                     0.199                     0.054   \n",
       "3  ...                     0.228                     0.045   \n",
       "4  ...                     0.193                     0.042   \n",
       "\n",
       "   Health_care_costs_Value  Could_not_see_doctor_due_to_cost_Value  \\\n",
       "0                    10127                                   0.163   \n",
       "1                     9939                                   0.156   \n",
       "2                     9502                                   0.144   \n",
       "3                    10414                                   0.169   \n",
       "4                    10825                                   0.163   \n",
       "\n",
       "   Other_primary_care_providers_Value  Median_household_income_Value  \\\n",
       "0                                  48                          42882   \n",
       "1                                  18                          51868   \n",
       "2                                  29                          47539   \n",
       "3                                  11                          30981   \n",
       "4                                   9                          39781   \n",
       "\n",
       "   Children_eligible_for_free_lunch_Value  Homicide_rate_Value  bottom10  \\\n",
       "0                                   0.500                  9.1         0   \n",
       "1                                   0.383                  4.6         0   \n",
       "2                                   0.344                  4.3         0   \n",
       "3                                   0.697                  NaN         0   \n",
       "4                                   0.546                  7.8         0   \n",
       "\n",
       "   top10  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%pylab inline\n",
    "\n",
    "from pylab import *\n",
    "\n",
    "# load statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# load the data\n",
    "path = r'CHR2015.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1524 1524 (3048, 71)\n"
     ]
    }
   ],
   "source": [
    "# add the response column \n",
    "df['aboveMedianPDV'] = [1 if d <= np.median(df['Premature_death_Value']) else 0 for d in df['Premature_death_Value']]\n",
    "print(len(df[df.aboveMedianPDV == 0]), len(df[df.aboveMedianPDV == 1]), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the null values in all columns to decide on whether it would be \n",
    "# beneficial to remove some of them to preserve most rows\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adult_smoking_Value          0\n",
       "Adult_obesity_Value          0\n",
       "Physical_inactivity_Value    0\n",
       "Diabetes_Value               0\n",
       "Teen_births_Value            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove NaNs\n",
    "df_fix = df[~np.isnan(df.Adult_smoking_Value) & ~np.isnan(df.Adult_obesity_Value) &\n",
    "           ~np.isnan(df.Physical_inactivity_Value) & ~np.isnan(df.Diabetes_Value) &\n",
    "            ~np.isnan(df.Teen_births_Value) & ~np.isnan(df.aboveMedianPDV)]\n",
    "df_fix = df_fix.reset_index(drop=True)\n",
    "df_fix[['Adult_smoking_Value', 'Adult_obesity_Value', 'Physical_inactivity_Value', \n",
    "             'Diabetes_Value', 'Teen_births_Value']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dataframes with an intercept column\n",
    "y, X = dmatrices('aboveMedianPDV ~ Adult_smoking_Value + Adult_obesity_Value + \\\n",
    "                 Physical_inactivity_Value + Diabetes_Value + \\\n",
    "                 Teen_births_Value',\n",
    "                df_fix, return_type=\"dataframe\")\n",
    "# flatten y into a 1-D array\n",
    "#y = np.ravel(y) #OR\n",
    "y = y.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412874583795783"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a logistic regression model, and fit with X and y\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X, y)\n",
    "\n",
    "# check the score\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50055493895671477"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what percentage had above median PDV\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=1)\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# predict class labels for the test set\n",
    "predicted = model2.predict(X_test)\n",
    "print (predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02240709  0.97759291]\n",
      " [ 0.0854317   0.9145683 ]\n",
      " [ 0.04864101  0.95135899]\n",
      " ..., \n",
      " [ 0.48537846  0.51462154]\n",
      " [ 0.63218017  0.36781983]\n",
      " [ 0.83999376  0.16000624]]\n"
     ]
    }
   ],
   "source": [
    "# generate class probabilities\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824399260628\n"
     ]
    }
   ],
   "source": [
    "# generate evaluation metrics\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "#print (metrics.roc_auc_score(y_test, probs[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[432 102]\n",
      " [ 88 460]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.81      0.82       534\n",
      "        1.0       0.82      0.84      0.83       548\n",
      "\n",
      "avg / total       0.82      0.82      0.82      1082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print (metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.87084871  0.7896679   0.74169742  0.86666667  0.85555556  0.81481481\n",
      "  0.81111111  0.84444444  0.85555556  0.81481481]\n",
      "0.82651769851\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   9.25724549e-02,   5.36061051e-01,\n",
       "          4.52388003e-01,   8.03939257e-01,   2.26519799e-01],\n",
       "       [  0.00000000e+00,   6.05725386e-02,   1.14802847e-01,\n",
       "          1.81452368e-01,   4.67677772e-01,  -3.15596377e-01],\n",
       "       [  0.00000000e+00,  -1.15427001e-01,  -1.28200067e+00,\n",
       "         -3.06231776e-01,   2.15481659e-01,   1.09592781e-01],\n",
       "       ..., \n",
       "       [  0.00000000e+00,  -5.95425746e-01,  -3.28626841e-01,\n",
       "         -4.50730782e-01,  -1.17159697e+00,   3.29549125e-03],\n",
       "       [  0.00000000e+00,  -1.17142424e+00,  -1.45937255e+00,\n",
       "         -5.22980284e-01,  -5.41106682e-01,  -1.45520714e-01],\n",
       "       [  0.00000000e+00,  -2.43426666e-01,  -4.39484263e-01,\n",
       "          8.28610650e-04,  -1.04549891e+00,   1.25537374e-01]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.scale(X, axis=0, with_mean=True, with_std=True, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# supress all warnings because LDA warns about collinearity \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84387717351091385"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA\n",
    "model = LDA()\n",
    "model = model.fit(X, y)\n",
    "\n",
    "# check the score\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDA(n_components=None, priors=None, shrinkage=None, solver='svd',\n",
       "  store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=1)\n",
    "model2 = LDA()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# predict class labels for the test set\n",
    "predicted = model2.predict(X_test)\n",
    "print (predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01069505  0.98930495]\n",
      " [ 0.0438153   0.9561847 ]\n",
      " [ 0.01643779  0.98356221]\n",
      " ..., \n",
      " [ 0.57334576  0.42665424]\n",
      " [ 0.90817245  0.09182755]\n",
      " [ 0.93983026  0.06016974]]\n"
     ]
    }
   ],
   "source": [
    "# generate class probabilities\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846580406654\n",
      "0.92162511277\n"
     ]
    }
   ],
   "source": [
    "# generate evaluation metrics\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[442  92]\n",
      " [ 74 474]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.83      0.84       534\n",
      "        1.0       0.84      0.86      0.85       548\n",
      "\n",
      "avg / total       0.85      0.85      0.85      1082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print (metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94095941  0.79335793  0.75645756  0.85555556  0.85925926  0.83333333\n",
      "  0.82222222  0.82592593  0.87037037  0.82962963]\n",
      "0.838707120405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/motech/anaconda/lib/python2.7/site-packages/sklearn/lda.py:371: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LDA(), X, y, scoring='accuracy', cv=10)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85275619681835002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "model = KNeighborsClassifier()\n",
    "model = model.fit(X, y)\n",
    "\n",
    "# check the score\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_neighbors=5, p=2, weights='uniform')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=1)\n",
    "model2 = KNeighborsClassifier()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  1.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# predict class labels for the test set\n",
    "predicted = model2.predict(X_test)\n",
    "print (predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   1. ]\n",
      " [ 0.   1. ]\n",
      " [ 0.   1. ]\n",
      " ..., \n",
      " [ 0.2  0.8]\n",
      " [ 0.4  0.6]\n",
      " [ 1.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "# generate class probabilities\n",
    "probs = model2.predict_proba(X_test)\n",
    "print (probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792975970425\n",
      "0.852613521419\n"
     ]
    }
   ],
   "source": [
    "# generate evaluation metrics\n",
    "print (metrics.accuracy_score(y_test, predicted))\n",
    "print (metrics.roc_auc_score(y_test, probs[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[418 116]\n",
      " [108 440]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.78      0.79       534\n",
      "        1.0       0.79      0.80      0.80       548\n",
      "\n",
      "avg / total       0.79      0.79      0.79      1082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print (metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80442804  0.75645756  0.73431734  0.83333333  0.84444444  0.78888889\n",
      "  0.8037037   0.82592593  0.81851852  0.76666667]\n",
      "0.797668443351\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(KNeighborsClassifier(), X, y, scoring='accuracy', cv=10)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Automate analysis to run on multiple sets of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up 3 datasets for automation\n",
    "# and remove NaNs\n",
    "\n",
    "# dataset 1\n",
    "df_fix1 = df[~np.isnan(df.Adult_smoking_Value) & ~np.isnan(df.Adult_obesity_Value) &\n",
    "           ~np.isnan(df.Physical_inactivity_Value) & ~np.isnan(df.Diabetes_Value) &\n",
    "            ~np.isnan(df.Teen_births_Value) & ~np.isnan(df.aboveMedianPDV)]\n",
    "df_fix1 = df_fix1.reset_index(drop=True)\n",
    "\n",
    "# create dataframes with an intercept column\n",
    "y1, X1 = dmatrices('aboveMedianPDV ~ Adult_smoking_Value + Adult_obesity_Value + \\\n",
    "                 Physical_inactivity_Value + Diabetes_Value + Teen_births_Value',\n",
    "                df_fix1, return_type=\"dataframe\")\n",
    "# flatten y into a 1-D array\n",
    "y1 = y1.values.flatten()\n",
    "\n",
    "# dataset 2\n",
    "df_fix2 = df[~np.isnan(df.Children_in_poverty_Value) & ~np.isnan(df.Income_inequality_Value) &\n",
    "           ~np.isnan(df.Unemployment_Value) & ~np.isnan(df.Severe_housing_problems_Value) &\n",
    "            ~np.isnan(df.Social_associations_Value) & ~np.isnan(df.aboveMedianPDV)]\n",
    "df_fix2 = df_fix2.reset_index(drop=True)\n",
    "\n",
    "# create dataframes with an intercept column\n",
    "y2, X2 = dmatrices('aboveMedianPDV ~ Children_in_poverty_Value + Income_inequality_Value + \\\n",
    "                 Unemployment_Value + Severe_housing_problems_Value + Social_associations_Value',\n",
    "                df_fix2, return_type=\"dataframe\")\n",
    "# flatten y into a 1-D array\n",
    "y2 = y2.values.flatten()\n",
    "\n",
    "# dataset 1\n",
    "df_fix3 = df[~np.isnan(df.Food_insecurity_Value) & ~np.isnan(df.Limited_access_to_healthy_foods_Value) &\n",
    "           ~np.isnan(df.Median_household_income_Value) & ~np.isnan(df.Health_care_costs_Value) &\n",
    "            ~np.isnan(df.Population_living_in_a_rural_area_Value) & ~np.isnan(df.aboveMedianPDV)]\n",
    "df_fix3 = df_fix3.reset_index(drop=True)\n",
    "\n",
    "# create dataframes with an intercept column\n",
    "y3, X3 = dmatrices('aboveMedianPDV ~ Food_insecurity_Value + Limited_access_to_healthy_foods_Value + \\\n",
    "                 Median_household_income_Value + Health_care_costs_Value + Population_living_in_a_rural_area_Value',\n",
    "                df_fix3, return_type=\"dataframe\")\n",
    "# flatten y into a 1-D array\n",
    "y3 = y3.values.flatten()\n",
    "\n",
    "datasets = {1:[X1,y1], 2:[X2,y2], 3:[X3,y3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare functions for automation\n",
    "import sklearn\n",
    "logistic = sklearn.linear_model.LogisticRegression()\n",
    "lda = sklearn.lda.LDA()\n",
    "knn = sklearn.neighbors.KNeighborsClassifier()\n",
    "functions = {1:logistic, 2:lda, 3:knn}\n",
    "ks = [5, 15, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to fit models\n",
    "def fitModel(datadict, functiondict):\n",
    "    for key in datadict:\n",
    "        for fun_key in functiondict:\n",
    "            pair = datadict[key]\n",
    "            X = pair[0]\n",
    "            y = pair[1]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=1)\n",
    "            model = functiondict[fun_key]\n",
    "            model = model.fit(X_train, y_train)\n",
    "            predicted = model.predict(X_test)\n",
    "            # generate class probabilities\n",
    "            probs = model.predict_proba(X_test)\n",
    "            # model accuracy\n",
    "            #print (metrics.accuracy_score(y_test, predicted))\n",
    "            # confusion matrix\n",
    "            print ('\\n')\n",
    "            print(functiondict[fun_key].__class__)\n",
    "            print('\\n'+\" --> dataset %d \" %(key))\n",
    "            print ('\\n'+ \"Confusion matrix:\"+'\\n')\n",
    "            print(metrics.confusion_matrix(y_test, predicted))\n",
    "            # evaluate the model using 10-fold cross-validation\n",
    "            scores = cross_val_score(functiondict[fun_key], X, y, scoring='accuracy', cv=10)\n",
    "            print ('\\n'+\"The model score:\"+'\\n')\n",
    "            print(scores.mean())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "\n",
      " --> dataset 1 \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[432 102]\n",
      " [ 88 460]]\n",
      "\n",
      "The model score:\n",
      "\n",
      "0.82651769851\n",
      "\n",
      "\n",
      "<class 'sklearn.lda.LDA'>\n",
      "\n",
      " --> dataset 1 \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[442  92]\n",
      " [ 74 474]]\n",
      "\n",
      "The model score:\n",
      "\n",
      "0.838707120405\n",
      "\n",
      "\n",
      "<class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "\n",
      " --> dataset 1 \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[418 116]\n",
      " [108 440]]\n",
      "\n",
      "The model score:\n",
      "\n",
      "0.797668443351\n",
      "\n",
      "\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "\n",
      " --> dataset 2 \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[463 156]\n",
      " [ 90 511]]\n",
      "\n",
      "The model score:\n",
      "\n",
      "0.789032937736\n",
      "\n",
      "\n",
      "<class 'sklearn.lda.LDA'>\n",
      "\n",
      " --> dataset 2 \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[464 155]\n",
      " [ 81 520]]\n",
      "\n",
      "The model score:\n",
      "\n",
      "0.797232972136\n",
      "\n",
      "\n",
      "<class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "\n",
      " --> dataset 2 \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[400 219]\n",
      " [150 451]]\n",
      "\n",
      "The model score:\n",
      "\n",
      "0.678459322325\n",
      "\n",
      "\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "\n",
      " --> dataset 3 \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[469 142]\n",
      " [118 489]]\n",
      "\n",
      "The model score:\n",
      "\n",
      "0.790444651689\n",
      "\n",
      "\n",
      "<class 'sklearn.lda.LDA'>\n",
      "\n",
      " --> dataset 3 \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[488 123]\n",
      " [127 480]]\n",
      "\n",
      "The model score:\n",
      "\n",
      "0.799958325579\n",
      "\n",
      "\n",
      "<class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "\n",
      " --> dataset 3 \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[470 141]\n",
      " [138 469]]\n",
      "\n",
      "The model score:\n",
      "\n",
      "0.771416267686\n"
     ]
    }
   ],
   "source": [
    "fitModel(datasets, functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Project Idea investigation : \n",
    "####Round 1.  Locate a data source that interests you (consider Datasets and Project Ideas link).  What data fields would be involved in your analysis ? Describe some of the Machine Learning techniques that could be used to analyzed this data. My expectation is that it will require at least a couple of paragraphs to address the above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Project Partner --> Sreedevi Gangarapu\n",
    "\n",
    "###Project Dataset: Yelp Dataset (from Yelp Dataset Challenge)\n",
    "\n",
    "    For our final project, Sreedevi and I decided to use the Yelp dataset with a possibility of submitting our work as a solution to their challenge. They offer prizes to students who can use their dataset in a creative and meaningful way.\n",
    "    The organizers of the challenge provide participants with some initial ideas and starting points that help decide what questions we want answered with the help of this dataset. We currently feel very interested in their suggestions for Natural Language Processing. There are several questions that Yelp organizers are interested in. First, we could try to predict the rating of the review from its text, which would require using some sentiment analysis technique or tool. Another question that interests Yelp is whether their reviewers are sarcastic. We found a description of a method someone used to determine whether an expression is colored with sarcasm. It requires collecting sentences from Twitter with a #sarcasm and using them for training of the \"sarcasm detector\". It is an interesting path for a project, but can be too complex for this class. Finally, The challenge organizers are interested in the most common positive an negative words used by their reviewers. That could be don using several sentiment analysis tools.\n",
    "    The dataset consists of 1.6M reviews and 500K tips by 366K users for 61K businesses with 481K business attributes, e.g., hours, parking availability, ambience, social network of 366K users for a total of 2.9M social edges, and aggregated check-ins over time for each of the 61K businesses. There are a lot of othere interesting questions that can be answered with dataset about seasonal and cultural trends. We are considering some of these question also potentially interesting and useful for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
